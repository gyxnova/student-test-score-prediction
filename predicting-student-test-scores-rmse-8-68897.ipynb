{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:05.859402Z","iopub.execute_input":"2026-01-29T12:06:05.860290Z","iopub.status.idle":"2026-01-29T12:06:05.870065Z","shell.execute_reply.started":"2026-01-29T12:06:05.860255Z","shell.execute_reply":"2026-01-29T12:06:05.869122Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ§  Problem Type\n\nThis is a **Supervised Learning** problem â€” specifically **Regression** (no classification drama here).\n\n---\n\n## ğŸ¯ Target Variable\n\nOur precious target column is:\n\n> **`exam_score`**\n> (the final boss weâ€™re trying to predict)\n\n---\n\n## ğŸ“ Evaluation Metric\n\nWeâ€™ll judge our model using:\n\n> **RMSE (Root Mean Squared Error)**\n\nWhy RMSE?\n\n* Penalizes big mistakes harder (strict teacher vibes ğŸ‘€)\n* Perfect for continuous values like exam scores\n\n---\n\n## ğŸ“ Dataset Shape\n\nThe dataset is *not small, not mediumâ€¦ itâ€™s Kaggle-big* ğŸ’ª\n\n```\nRows Ã— Columns = 630,000 Ã— 13\n```\n\nThatâ€™s:\n\n* Enough data to train a serious model\n* Enough data to accidentally mess up RAM if not careful ğŸ˜…\n\n---\n\n## ğŸš¨ Data Leakage (The Crime Scene ğŸ•µï¸â€â™€ï¸)\n\n**Data leakage is when your model secretly cheats in the exam**\nâ€” just like knowing the answer key before the test ğŸ“âŒ\n\nExamples of leakage we *absolutely avoided*:\n\n* âŒ Using future information\n* âŒ Letting `exam_score` influence features\n* âŒ Splitting data *after* feature engineering\n\n### ğŸ” The Suspect: `Id`\n\nThe `Id` column tried to act innocent, butâ€¦\n\n> **Id knows nothing about exam scores.\n> Itâ€™s just a roll number pretending to be useful.**\n\nSo we politely showed it the exit ğŸšªğŸ™‚\n\n---\n\nNow let the model **study properly** and earn its marks ğŸ“šğŸ”¥\n\n---\n","metadata":{}},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:05.886162Z","iopub.execute_input":"2026-01-29T12:06:05.886452Z","iopub.status.idle":"2026-01-29T12:06:05.905009Z","shell.execute_reply.started":"2026-01-29T12:06:05.886426Z","shell.execute_reply":"2026-01-29T12:06:05.903949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset \ndf = pd.read_csv('/kaggle/input/playground-series-s6e1/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s6e1/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:05.906360Z","iopub.execute_input":"2026-01-29T12:06:05.906660Z","iopub.status.idle":"2026-01-29T12:06:07.178030Z","shell.execute_reply.started":"2026-01-29T12:06:05.906633Z","shell.execute_reply":"2026-01-29T12:06:07.177191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:07.179874Z","iopub.execute_input":"2026-01-29T12:06:07.180234Z","iopub.status.idle":"2026-01-29T12:06:07.186105Z","shell.execute_reply.started":"2026-01-29T12:06:07.180205Z","shell.execute_reply":"2026-01-29T12:06:07.185032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:07.187570Z","iopub.execute_input":"2026-01-29T12:06:07.187890Z","iopub.status.idle":"2026-01-29T12:06:07.447775Z","shell.execute_reply.started":"2026-01-29T12:06:07.187864Z","shell.execute_reply":"2026-01-29T12:06:07.446905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:07.448818Z","iopub.execute_input":"2026-01-29T12:06:07.449167Z","iopub.status.idle":"2026-01-29T12:06:07.841989Z","shell.execute_reply.started":"2026-01-29T12:06:07.449129Z","shell.execute_reply":"2026-01-29T12:06:07.840900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:07.843202Z","iopub.execute_input":"2026-01-29T12:06:07.843558Z","iopub.status.idle":"2026-01-29T12:06:07.986291Z","shell.execute_reply.started":"2026-01-29T12:06:07.843524Z","shell.execute_reply":"2026-01-29T12:06:07.985510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot histogram of numeric cols\ndf.hist(bins=50,figsize=(25,20))\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:07.987414Z","iopub.execute_input":"2026-01-29T12:06:07.987983Z","iopub.status.idle":"2026-01-29T12:06:09.189509Z","shell.execute_reply.started":"2026-01-29T12:06:07.987944Z","shell.execute_reply":"2026-01-29T12:06:09.188612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['high_att+study']= ((df['class_attendance']>=90) &(df['study_hours']>=6)).astype(int)\ndf['ideal_sleep'] = ((df['sleep_hours']>=7) & (df['sleep_hours']<=9)).astype(int)\ndf['ideal_study'] = (df['study_hours']>=7).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:09.191945Z","iopub.execute_input":"2026-01-29T12:06:09.192252Z","iopub.status.idle":"2026-01-29T12:06:09.209893Z","shell.execute_reply.started":"2026-01-29T12:06:09.192226Z","shell.execute_reply":"2026-01-29T12:06:09.208932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:09.429097Z","iopub.execute_input":"2026-01-29T12:06:09.429453Z","iopub.status.idle":"2026-01-29T12:06:09.436298Z","shell.execute_reply.started":"2026-01-29T12:06:09.429417Z","shell.execute_reply":"2026-01-29T12:06:09.434927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df.drop(columns=['id','exam_score'])\ny = df['exam_score']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:09.437510Z","iopub.execute_input":"2026-01-29T12:06:09.437854Z","iopub.status.idle":"2026-01-29T12:06:09.540855Z","shell.execute_reply.started":"2026-01-29T12:06:09.437814Z","shell.execute_reply":"2026-01-29T12:06:09.539999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split data\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size = 0.2,random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:09.542136Z","iopub.execute_input":"2026-01-29T12:06:09.542497Z","iopub.status.idle":"2026-01-29T12:06:09.843278Z","shell.execute_reply.started":"2026-01-29T12:06:09.542458Z","shell.execute_reply":"2026-01-29T12:06:09.842142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:09.844627Z","iopub.execute_input":"2026-01-29T12:06:09.845022Z","iopub.status.idle":"2026-01-29T12:06:09.850882Z","shell.execute_reply.started":"2026-01-29T12:06:09.844983Z","shell.execute_reply":"2026-01-29T12:06:09.850122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_val.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:09.851946Z","iopub.execute_input":"2026-01-29T12:06:09.852231Z","iopub.status.idle":"2026-01-29T12:06:09.876313Z","shell.execute_reply.started":"2026-01-29T12:06:09.852206Z","shell.execute_reply":"2026-01-29T12:06:09.875131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# seperate numerical and categorical columns\nnum_col = pd.DataFrame(X_train.select_dtypes(include=['int64','float64']))\ncat_col = X_train.select_dtypes(include=['object']).columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:09.877496Z","iopub.execute_input":"2026-01-29T12:06:09.877990Z","iopub.status.idle":"2026-01-29T12:06:09.987263Z","shell.execute_reply.started":"2026-01-29T12:06:09.877954Z","shell.execute_reply":"2026-01-29T12:06:09.986248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# value_count for categorical data\nfor col in cat_col:\n    print(df[col].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:09.988403Z","iopub.execute_input":"2026-01-29T12:06:09.988770Z","iopub.status.idle":"2026-01-29T12:06:10.342068Z","shell.execute_reply.started":"2026-01-29T12:06:09.988741Z","shell.execute_reply":"2026-01-29T12:06:10.340886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_col.corrwith(y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:10.343259Z","iopub.execute_input":"2026-01-29T12:06:10.343611Z","iopub.status.idle":"2026-01-29T12:06:10.490954Z","shell.execute_reply.started":"2026-01-29T12:06:10.343573Z","shell.execute_reply":"2026-01-29T12:06:10.489843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import seaborn as sns \n# sns.pairplot(num_col)\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:10.492231Z","iopub.execute_input":"2026-01-29T12:06:10.492548Z","iopub.status.idle":"2026-01-29T12:06:10.497482Z","shell.execute_reply.started":"2026-01-29T12:06:10.492516Z","shell.execute_reply":"2026-01-29T12:06:10.496545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for col in num_col:\n#     plt.figure(figsize=(30,10))\n#     sns.boxplot(x=num_col[col],y=y_train)\n#     plt.show","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:10.498645Z","iopub.execute_input":"2026-01-29T12:06:10.499151Z","iopub.status.idle":"2026-01-29T12:06:10.516109Z","shell.execute_reply.started":"2026-01-29T12:06:10.499114Z","shell.execute_reply":"2026-01-29T12:06:10.515152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ordinal_col = ['internet_access','sleep_quality','facility_rating','exam_difficulty']\n\nnormina_col = [col for col in cat_col if col not in ordinal_col ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:10.517266Z","iopub.execute_input":"2026-01-29T12:06:10.517940Z","iopub.status.idle":"2026-01-29T12:06:10.539158Z","shell.execute_reply.started":"2026-01-29T12:06:10.517889Z","shell.execute_reply":"2026-01-29T12:06:10.538318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸš¨ Outlier Handling â€” *Standard Deviation Edition*\n\nSome features decided to go **too far from the mean** and risk ruining our RMSE.\nInstead of panicking, we handled them **statistically and calmly** ğŸ§˜â€â™€ï¸\n\n---\n\n### ğŸ§ª Strategy Used: **Standard Deviationâ€“Based Capping**\n\nOutliers are capped using the formula:\n\n[\n{Lower Bound} = \\mu - k\\sigma \\\n{Upper Bound} = \\mu + k\\sigma\n]\n\nWhere:\n\n* (\\mu) = feature mean\n* (\\sigma) = feature standard deviation\n* (k = 3) (default)\n\nThis assumes features are **approximately normally distributed** â€” fair enough for exam-related data ğŸ“Š\n\n---\n\n### ğŸ› ï¸ Implementation Details\n\n* A **custom scikit-learn transformer** (`CappingTransformer`) was created\n* Compatible with **pipelines & cross-validation** âœ”ï¸\n* Bounds are **learned only from training data** (no cheating ğŸ‘€)\n* Same bounds are applied consistently during inference\n\n---\n\n### âœ‚ï¸ What Exactly Happens?\n\n* âŒ No rows are dropped\n* âŒ No data leakage\n* âœ… Values beyond Â±3Ïƒ are **clipped**\n* ğŸ¯ Target column (`exam_score`) is **not transformed**\n\nOutliers were told:\n\n> â€œYouâ€™re still part of the dataset â€” just stay within 3Ïƒ.â€\n\n---\n\n### ğŸ¤” Why Capping Instead of Removal?\n\n* Large dataset (**630k rows**) â†’ keep information\n* Prevents extreme values from dominating loss\n* RMSE stays stable\n* Model stays sane\n\n---\n\nData is now **clean, disciplined, and model-ready** ğŸš€\n\n---\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass CappingTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    A custom transformer for capping outliers in a normal distribution \n    based on standard deviations within a scikit-learn pipeline.\n    \n    Attributes:\n        std_devs (float): The number of standard deviations to use for bounds.\n        upper_bounds_ (dict): Stored upper bounds for each feature during fit.\n        lower_bounds_ (dict): Stored lower bounds for each feature during fit.\n    \"\"\"\n    def __init__(self, std_devs=3.0):\n        self.std_devs = std_devs\n        self.upper_bounds_ = {}\n        self.lower_bounds_ = {}\n\n    def fit(self, X, y=None):\n        \"\"\"Calculates the capping bounds based on the input data.\"\"\"\n        # Check if X is a pandas DataFrame and convert to numpy array if not for consistency\n        if not isinstance(X, pd.DataFrame):\n            X = pd.DataFrame(X)\n            \n        for col in X.columns:\n            mean = X[col].mean()\n            std = X[col].std()\n            self.upper_bounds_[col] = mean + (self.std_devs * std)\n            self.lower_bounds_[col] = mean - (self.std_devs * std)\n            \n        return self\n\n    def transform(self, X):\n        \"\"\"Applies the capping bounds to the input data.\"\"\"\n        # Ensure transformation data matches the structure of fitting data\n        if not isinstance(X, pd.DataFrame):\n            X = pd.DataFrame(X)\n            \n        X_capped = X.copy()\n        for col in X_capped.columns:\n            # Clip values: values above upper bound become the upper bound, \n            # and values below the lower bound become the lower bound.\n            X_capped[col] = np.clip(X_capped[col], \n                                    self.lower_bounds_[col], \n                                    self.upper_bounds_[col])\n            \n        return X_capped\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:10.540278Z","iopub.execute_input":"2026-01-29T12:06:10.540609Z","iopub.status.idle":"2026-01-29T12:06:10.562256Z","shell.execute_reply.started":"2026-01-29T12:06:10.540583Z","shell.execute_reply":"2026-01-29T12:06:10.561349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outlier = ['study_hours','class_attendance','ideal_study','high_att+study']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:10.563322Z","iopub.execute_input":"2026-01-29T12:06:10.563577Z","iopub.status.idle":"2026-01-29T12:06:10.585402Z","shell.execute_reply.started":"2026-01-29T12:06:10.563553Z","shell.execute_reply":"2026-01-29T12:06:10.584597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_pipe = Pipeline([\n    ('imputer',SimpleImputer(strategy='mean')),\n    ('scale',StandardScaler())\n])\n\nord_pipe =Pipeline([\n    ('imputer',SimpleImputer(strategy='most_frequent')),\n    ('encoder',OrdinalEncoder(\n        categories=[\n         \n                    ['yes','no'],\n                    ['good','average','poor'],\n                    ['high','medium','low'],\n                    ['easy','moderate','hard']\n         \n        ]\n    ))\n])\n\nnor_pipe = Pipeline([\n    ('imputer',SimpleImputer(strategy='most_frequent')),\n    ('encoder',OneHotEncoder(handle_unknown='ignore'))\n])\n\nout_pipe = Pipeline([\n     ('capper', CappingTransformer(std_devs=3.0)),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:10.590061Z","iopub.execute_input":"2026-01-29T12:06:10.590610Z","iopub.status.idle":"2026-01-29T12:06:10.610026Z","shell.execute_reply.started":"2026-01-29T12:06:10.590569Z","shell.execute_reply":"2026-01-29T12:06:10.608959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_col = X_train.select_dtypes(include=['int64','float64']).columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:10.611225Z","iopub.execute_input":"2026-01-29T12:06:10.611658Z","iopub.status.idle":"2026-01-29T12:06:10.683246Z","shell.execute_reply.started":"2026-01-29T12:06:10.611629Z","shell.execute_reply":"2026-01-29T12:06:10.682363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocessor = ColumnTransformer([\n    ('out', out_pipe, outlier),\n    ('num', num_pipe, num_col),\n    ('ord', ord_pipe, ordinal_col),\n    ('nom', nor_pipe, normina_col)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:10.684405Z","iopub.execute_input":"2026-01-29T12:06:10.684822Z","iopub.status.idle":"2026-01-29T12:06:10.689971Z","shell.execute_reply.started":"2026-01-29T12:06:10.684706Z","shell.execute_reply":"2026-01-29T12:06:10.688436Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ¤– Model â€” LightGBM Regressor (The Workhorse)\n\nFor this task, we use **LightGBM**, a gradient boosting framework known for:\n\n* High performance on large tabular datasets\n* Fast training\n* Strong handling of non-linear relationships\n\nPerfect fit for **630k rows** ğŸ’ª\n\n---\n\n### ğŸ§© Pipeline Setup\n\nThe model is wrapped inside a **scikit-learn Pipeline**, ensuring:\n\n* All preprocessing happens **inside CV / train-val split**\n* Zero data leakage\n* Clean, reproducible workflow\n\nPipeline structure:\n\n* **Preprocessor** â†’ feature transformations\n* **LGBMRegressor** â†’ prediction engine\n\n---\n\n### âš™ï¸ Model Configuration (Key Choices)\n\n* **n_estimators = 5000**\n  Allows the model to learn complex patterns with a small learning rate\n\n* **learning_rate = 0.03**\n  Slow and steady learning = better generalization\n\n* **max_depth = 5**\n  Controls overfitting by limiting tree complexity\n\n* **num_leaves = 31**\n  Balanced expressive power without becoming noisy\n\n* **min_data_in_leaf = 40**\n  Prevents the model from memorizing tiny patterns\n\n---\n\n### ğŸ›¡ï¸ Regularization & Stability\n\nTo keep the model disciplined:\n\n* **L1 Regularization (`lambda_l1 = 0.2`)**\n* **L2 Regularization (`lambda_l2 = 0.4`)**\n* **Feature sampling (`feature_fraction = 0.85`)**\n* **Row sampling (`bagging_fraction = 0.85`, `bagging_freq = 1`)**\n\nTranslation:\n\n> The model is smart, not reckless ğŸ˜Œ\n\n---\n\n### ğŸ“ Evaluation Metric\n\nModel performance is evaluated using:\n\n> **RMSE (Root Mean Squared Error)**\n\n* Computed on both **training** and **validation** sets\n* Helps monitor overfitting\n* Penalizes large prediction errors\n\n---\n\n### ğŸ“‰ Results Interpretation\n\n* **Train RMSE** â†’ how well the model fits known data\n* **Validation RMSE** â†’ how well it generalizes\n\nA small gap between the two indicates:\n\n* Good generalization\n* No major overfitting\n\n---\n\nModel is **trained, tested, and ready to compete** ğŸš€\n\n---\n","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\n\nrf_model = Pipeline([\n    ('prep',preprocessor),\n    ('model',LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.03,\n        max_depth=5,\n        num_leaves=31,\n        min_data_in_leaf=40,\n        lambda_l1=0.2,\n        lambda_l2=0.4,\n        feature_fraction=0.85,\n        bagging_fraction=0.85,\n        bagging_freq=1,\n        random_state=42,\n        n_jobs=-1,\n        verbosity=-1))\n\n])\n\nrf_model.fit(X_train, y_train)\n\n\ny_test_prob = rf_model.predict(X_val)\ny_train_prob = rf_model.predict(X_train)\n\nrmse_val =  np.sqrt(mean_squared_error(y_val, y_test_prob))\nrmse_train =  np.sqrt(mean_squared_error(y_train, y_train_prob))\n\nprint(\"Train RMSE:\", rmse_train)\nprint(\"Test RMSE:\", rmse_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:06:10.751890Z","iopub.execute_input":"2026-01-29T12:06:10.752217Z","iopub.status.idle":"2026-01-29T12:10:57.136442Z","shell.execute_reply.started":"2026-01-29T12:06:10.752180Z","shell.execute_reply":"2026-01-29T12:10:57.135393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test['high_att+study'] = (\n    (test['class_attendance'] >= 90) &\n    (test['study_hours'] >= 6)\n).astype(int)\n\ntest['ideal_sleep'] = (\n    (test['sleep_hours'] >= 7) &\n    (test['sleep_hours'] <= 9)\n).astype(int)\n\ntest['ideal_study'] = (\n    test['study_hours'] >= 7\n).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:10:57.137950Z","iopub.execute_input":"2026-01-29T12:10:57.138698Z","iopub.status.idle":"2026-01-29T12:10:57.151948Z","shell.execute_reply.started":"2026-01-29T12:10:57.138643Z","shell.execute_reply":"2026-01-29T12:10:57.150854Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_proba = rf_model.predict(test)\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'exam_score':test_proba\n})\nsubmission.to_csv('submission3.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T12:10:57.179367Z","iopub.execute_input":"2026-01-29T12:10:57.179882Z","iopub.status.idle":"2026-01-29T12:10:57.201408Z","shell.execute_reply.started":"2026-01-29T12:10:57.179830Z","shell.execute_reply":"2026-01-29T12:10:57.200408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ Final Results & Looking for Feedback\n\nAfter extensive experimentation and tuning, the **best score Iâ€™ve been able to achieve** in this competition is:\n\n> **ğŸ† RMSE: 8.68897**\n\n---\n\n### ğŸ” What Iâ€™ve Tried So Far\n\nTo push the score further, I experimented with:\n\n* LightGBM hyperparameter tuning\n* Different preprocessing strategies\n* Feature engineering ideas\n* Regularization adjustments\n* Ensembling techniques\n\nDespite these efforts, I havenâ€™t been able to achieve any **meaningful improvement** beyond this point.\n\n---\n\n### ğŸ“‰ Current Situation\n\nAt the moment, it feels like Iâ€™ve hit a **performance plateau**.\nFurther tuning either gives negligible gains or makes validation unstable.\n\nThis makes me think:\n\n* I may be missing some **important feature insight**\n* Or there might be **data limitations** Iâ€™m not seeing clearly\n\n---\n\n### ğŸ§  About My Approach\n\nIâ€™ve focused purely on **traditional machine learning models**, mainly tree-based methods like LightGBM.\nI havenâ€™t explored deep learning yet, as my current knowledge is limited to ML, and DL feels out of reach for now.\n\n---\n\n### ğŸ™ Asking for Help\n\nIâ€™d genuinely appreciate feedback from the community on:\n\n* Feature engineering ideas I might have missed\n* Validation strategy improvements\n* Whether this plateau is expected for this dataset\n* Any LightGBM tuning suggestions worth trying\n\nIf you see something obvious (or non-obvious!) that I overlooked, Iâ€™d love to learn from it.\n\n---\n\n### âœ… Closing Note\n\nThis notebook represents my **best effort so far**, but I know thereâ€™s room to grow.\nAny constructive feedback or suggestions would mean a lot.\n\nThank you for taking the time to read ğŸ™Œ\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}